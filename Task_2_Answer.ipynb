{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Контентная рекомендательная система"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ты уже запрограммировал систему неперсональных рекомендаций, в этом занятии ты пройдешь все этапы разработки модели контентных рекомендаций, а значит перейдешь на следующий уровень кодинга. \n",
    "\n",
    "Какой у нас план?\n",
    "\n",
    " - Добавим необходимые библиотеки и загрузим датасеты «movies_metadata_fixed.csv», «credits.csv», «keywords.csv», «links_small.csv». \n",
    " - Соберем в один датасет необходимые данные, добавим поля с актерами фильмов и ключевыми словами.\n",
    " - Обработаем данные, чтобы они были представлены в удобном виде и удалим ненужные элементы. Переведем описание фильмов в текстовый формат (в колонках представим набор слов).\n",
    " - Чтобы появилась возможность сравнивать описания фильмов, превратим текст в вектор. \n",
    " - Найдем и сравним похожие фильмы по их **векторному представлению**. \n",
    "\n",
    " Поехали!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начни с импорта библиотек, подгружаем **Pandas** и **Ast** (с пакетом Literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#импортируй библиотеки, которые мы использовали для решения первого задания\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import pandas as pd\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этот раз ты работаешь не только с таблицами, но и с текстами, а также с математическими объектами. Импортируем подходящие библиотеки и пакеты: **Scikit-learn** (пакеты CountVectorizer и cosine_similarity), **NLTK** (пакет SnowballStemmer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем датасет из файла **«movies_metadata_fixed.csv»**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Загрузи файл movies_metadata_fixed.csv в переменную dataset\n",
    "dataset = pd.read_csv('movies_metadata_fixed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично первому занятию подготовь колонки **Жанры** и **Год выпуска фильма**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Обработай жанры в датасете\n",
    "dataset['genres'] = dataset['genres'].fillna('[]').apply(literal_eval).apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Отдели год выпуска фильма от полной даты выхода\n",
    "dataset['year'] = pd.to_datetime(dataset['release_date'],errors='coerce').dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме информации о фильмах нам нужны будут дополнительные данные, их мы загрузим из других файлов.\n",
    "\n",
    "В файле **credits.csv** хранится информация о съемочной группе, а база **keywords.csv** содержит информацию о ключевых словах-  это слова-теги, которые характеризуют каждый из фильмов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "credits = pd.read_csv('credits.csv')\n",
    "keywords = pd.read_csv('keywords.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы прикрепить колонки с нужной информацией к рабочему датасету с фильмами из последних файлов, воспользуемся функцией **merge** из пакета **Pandas**. Объединяем колонки по столбцу \"id\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46628, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.merge(dataset, credits, on='id')\n",
    "dataset = pd.merge(dataset, keywords, on='id')\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы ускорить вычисления и быстро создать прототип рекомендательной системы, создатели датасета подготовили выборку из 9219 фильмов **links_small.csv**. На этом наборе данных ты быстрее протестируешь идеи, а уже потом сможешь применить ко всей базе фильмов целиком. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_small = pd.read_csv('links_small.csv')\n",
    "links_small = links_small[links_small['tmdbId'].notnull()]['tmdbId'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9219, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smd = dataset[dataset['id'].isin(links_small)]\n",
    "smd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видишь, наш датасет заметно уменьшился, можем переходить к обработке данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных\n",
    "\n",
    "Перед тобой стоит задача сравнить фильмы между собой, чтобы подобрать наиболее похожие. Как ты будешь определять эту \"похожесть\"? \n",
    "\n",
    "Есть пара идей:\n",
    " - Фильмы мог снять один и тот же режиссер\n",
    " - В фильмах сыграли одни и те же актеры\n",
    " - Наконец, сюжеты фильмов совпадают или один фильм продолжает другой\n",
    " \n",
    "Приведем в пример трилогию \"Хоббит\". Три части киноистории снял одним и тот же режиссер, актерский состав почти полностью совпадает, да и сюжет второй и третьей частей продолжают первую.\n",
    "\n",
    "Как раз эту информацию ты и попроббуешь собрать из нескольких колонок, а затем превратить в текст. Сейчас мы используем только эти поля из базы данных для примера, но есть и другие параметры, по которым фильмы могут совпадать (по смыслу). Ты можешь самостоятельно поработать с ними, когда завершишь общую часть второго задания.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начни с переменной **cast**. Этот формат ты уже использовал в первом задании, поэтому знаешь, как с ним справиться. Сначала примени функцию **literal_eval**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [{'cast_id': 14, 'character': 'Woody (voice)',...\n",
       "1    [{'cast_id': 1, 'character': 'Alan Parrish', '...\n",
       "2    [{'cast_id': 2, 'character': 'Max Goldman', 'c...\n",
       "3    [{'cast_id': 1, 'character': 'Savannah 'Vannah...\n",
       "4    [{'cast_id': 1, 'character': 'George Banks', '...\n",
       "Name: cast, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smd['cast'] = smd['cast'].apply(literal_eval)\n",
    "smd.cast.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На выходе **literal_eval** получаем список словарей в виде объектов языка Python. Информация, которая нам нужна, лежит в полях **name**. Сформируй список актерского состава тем же способом, что и в первом задании"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Tom Hanks, Tim Allen, Don Rickles, Jim Varney...\n",
       "1    [Robin Williams, Jonathan Hyde, Kirsten Dunst,...\n",
       "2    [Walter Matthau, Jack Lemmon, Ann-Margret, Sop...\n",
       "3    [Whitney Houston, Angela Bassett, Loretta Devi...\n",
       "4    [Steve Martin, Diane Keaton, Martin Short, Kim...\n",
       "Name: cast, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smd['cast'] = smd['cast'].apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])#Сформируй список актерского состава, используя лямбда-функцию\n",
    "smd.cast.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично! Теперь для каждого фильма у тебя есть список актеров. Теперь это поле мы превратим в одну большую строку. Предлагаем учитывать только главные роли фильма, оставим первые пять имен актеров, а остальные опустим.\n",
    "\n",
    "Чтобы отличать актеров с одинаковыми именами, но разными фамилиями, переведи строку в нижний регистр и убери пробелы в именах. Имя **Tom Hanks** запишется как **tomhanks**, тоже самое произойдет и с другими. Проделав эту операцию, алгоритм уже не будет считать фильмы похожими, просто потому что у актеров одинаковые имена."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    tomhanks timallen donrickles jimvarney wallace...\n",
       "1    robinwilliams jonathanhyde kirstendunst bradle...\n",
       "2    waltermatthau jacklemmon ann-margret sophialor...\n",
       "3    whitneyhouston angelabassett lorettadevine lel...\n",
       "4    stevemartin dianekeaton martinshort kimberlywi...\n",
       "Name: cast_str, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smd['cast'] = smd['cast'].apply(lambda x: x[:5] if len(x) >= 5 else x)#оставь только пять первых актеров\n",
    "smd['cast'] = smd['cast'].apply(lambda x: [str.lower(i.replace(' ', '')) for i in x])#переведи в нижний регистр имена и удали пробелы между именем и фамилией\n",
    "smd['cast_str'] = smd['cast'].apply(lambda x: ' '.join(x))#объедини актеров каждого фильма в одну строку, разделяя имена пробелами\n",
    "smd.cast_str.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На очереди колонка **crew**. В ней хранится важная информация, а именно в поле **job** — здесь есть имена и должности всех людей, которые принимали участие в съемках фильма. Давай используем функцию **literal_eval** и рассмотрим поле в деталях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [{'credit_id': '52fe4284c3a36847f8024f49', 'de...\n",
       "1    [{'credit_id': '52fe44bfc3a36847f80a7cd1', 'de...\n",
       "2    [{'credit_id': '52fe466a9251416c75077a89', 'de...\n",
       "3    [{'credit_id': '52fe44779251416c91011acb', 'de...\n",
       "4    [{'credit_id': '52fe44959251416c75039ed7', 'de...\n",
       "Name: crew, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smd['crew'] = smd['crew'].apply(literal_eval)#используй функцию literal_eval\n",
    "smd.crew.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'credit_id': '52fe4284c3a36847f8024f49',\n",
       " 'department': 'Directing',\n",
       " 'gender': 2,\n",
       " 'id': 7879,\n",
       " 'job': 'Director',\n",
       " 'name': 'John Lasseter',\n",
       " 'profile_path': '/7EdqiNbr4FRjIhKHyPPdFfEEEFG.jpg'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smd['crew'].iloc[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имя режиссера ты найдешь в поле **Director** (важно! это поле может отсутствовать). Напиши такую функцию, чтобы извлечь имя режиссера для каждого фильма, а если информации нет, то функция возвращает значение **np.nan**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      johnlasseter\n",
       "1       joejohnston\n",
       "2      howarddeutch\n",
       "3    forestwhitaker\n",
       "4      charlesshyer\n",
       "Name: director, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_director(x):\n",
    "    for i in x:\n",
    "        if i['job'] == 'Director':#проверьте, что должность - режиссер:\n",
    "            return i['name']#верните ответом имя режиссера\n",
    "    return np.nan\n",
    "smd['director'] = smd['crew'].apply(get_director)#примените к колонке с командой функцию поиска режиссера \n",
    "smd['director'] = smd['director'].astype('str').apply(lambda x: str.lower(x.replace(\" \", \"\")))\n",
    "smd.director.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последняя колонка, которая тебе нужна - это **keywords**, те самые ключевые слова-теги. Давай посмотрим, как она выглядит. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 931, 'name': 'jealousy'},\n",
       " {'id': 4290, 'name': 'toy'},\n",
       " {'id': 5202, 'name': 'boy'},\n",
       " {'id': 6054, 'name': 'friendship'},\n",
       " {'id': 9713, 'name': 'friends'},\n",
       " {'id': 9823, 'name': 'rivalry'},\n",
       " {'id': 165503, 'name': 'boy next door'},\n",
       " {'id': 170722, 'name': 'new toy'},\n",
       " {'id': 187065, 'name': 'toy comes to life'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smd['keywords'].head().apply(literal_eval).iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы извлечь данные, обратимся к полю **name** в каждом элементе списка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [jealousy, toy, boy, friendship, friends, riva...\n",
       "1    [board game, disappearance, based on children'...\n",
       "2    [fishing, best friend, duringcreditsstinger, o...\n",
       "3    [based on novel, interracial relationship, sin...\n",
       "4    [baby, midlife crisis, confidence, aging, daug...\n",
       "Name: keywords, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smd['keywords'] = smd['keywords'].apply(literal_eval)\n",
    "smd['keywords'] = smd['keywords'].apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])\n",
    "smd.keywords.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ты хочешь рекомендовать похожие фильмы, а значит есть смысл, оставить только те ключевые слова, которые встречаются у нескольких фильмов. Исключим ключевые слова, которые встречаются лишь раз во всем датасете:\n",
    "- соберем все ключевые слова всех фильмов в один большой список\n",
    "- посчитаем, сколько раз встречалось каждое ключевое слово\n",
    "- удалим те ключевые слова, которые встретились только раз"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = smd.apply(lambda x: pd.Series(x['keywords']),axis=1)\\\n",
    "       .stack()\\\n",
    "       .reset_index(level=1, drop=True)\n",
    "s.name = 'keyword'\n",
    "s = s.value_counts()\n",
    "s = s[s > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [jealousy, toy, boy, friendship, friends, riva...\n",
       "1    [board game, disappearance, based on children'...\n",
       "2         [fishing, best friend, duringcreditsstinger]\n",
       "3    [based on novel, interracial relationship, sin...\n",
       "4    [baby, midlife crisis, confidence, aging, daug...\n",
       "Name: keywords, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_keywords(x):\n",
    "    words = []\n",
    "    for i in x:\n",
    "        if i in s:\n",
    "            words.append(i)\n",
    "    return words\n",
    "smd['keywords'] = smd['keywords'].apply(filter_keywords)\n",
    "smd.keywords.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ты собрал список слов-тегов для каждого фильма, но это еще не всё. Если вчитаться в список, то некоторые ключевые слова повторяются, хотя имеют разные формы (например, единственное и множественное число)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 88)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s['friend'], s['friends']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s['boy'], s['boys']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представь, в фильме есть ключевые слова **friend и boy**, а в другом **friends и boys**. Алгоритм будет считать эти фильмы непохожими, а значит мы потеряем информацию.\n",
    "\n",
    "Чтобы разобраться с этой проблемой, используй специальный алгоритм - **стеммер** (stemmer) от англ. слова **stem** - корень.  \n",
    "Этот алгоритм поможет тебе выделить корень слова и превратить всех **friends** во **friend**, а **boys** в **boy**.\n",
    "\n",
    "Посмотри, как это работает:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('dog', 'friend', 'boy')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "stemmer.stem('dogs'), stemmer.stem('friends'), stemmer.stem('boys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм сработал, как нужно: для твоей задачи вполне подходит. Давай обработаем каждое ключевое слово для каждого фильма этим алгоритмом. Это улучшит результат поиска похожих фильмов (скоро сам в этом убедишься). \n",
    "\n",
    "Удобнее всего такое преобразование выразить как отдельную функцию. Напиши функцию, чтобы на входе она принимала список слов, а на выходе возращала уже обработанный."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_keywords(x):\n",
    "    stemmed_tokens = []\n",
    "    for token in x:\n",
    "        try:\n",
    "            new_token = stemmer.stem(token)\n",
    "            stemmed_tokens.append(new_token)\n",
    "        except:\n",
    "            stemmed_tokens.append(token)\n",
    "    return stemmed_tokens\n",
    "\n",
    "smd['keywords'] = smd['keywords'].apply(lambda x: stem_keywords(x))#примените написанную выше функцию ко всему столбцу\n",
    "smd['keywords'] = smd['keywords'].apply(lambda x: [i.replace(\" \", \"\").lower() for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    jealousi toy boy friendship friend rivalri boy...\n",
       "1    boardgam disappear basedonchildren'sbook newho...\n",
       "2                   fish bestfriend duringcreditssting\n",
       "3    basedonnovel interracialrelationship singlemot...\n",
       "4    babi midlifecrisi confid age daughter motherda...\n",
       "Name: keywords_str, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smd['keywords_str'] = smd['keywords'].apply(lambda x: ' '.join([str(i) for i in x]))\n",
    "smd.keywords_str.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Векторизация текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вау! Видим, что ты добрался практически до финиша разработки контентной системы рекомендаций. Все колонки, что ты так кропотливо готовил, пора направить в дело: keywords, director, cast, genres.\n",
    "\n",
    "Нужно объединить эти колонки в одну большую строку для каждого фильма, так у каждого объекта будет подробное текстовое описание. Объединяй колонки через пробел (чтобы слова не слиплись) и переводи все в нижний регистр."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_fields(data):\n",
    "    concat = data['keywords'] + data['cast'] + [data['director']] + data['genres']\n",
    "    result = ' '.join([str(i).lower() for i in concat])\n",
    "    return result\n",
    "smd['soup'] = smd.apply(lambda x: concat_fields(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Toy Story',\n",
       " 'jealousi toy boy friendship friend rivalri boynextdoor newtoy toycomestolif tomhanks timallen donrickles jimvarney wallaceshawn johnlasseter animation comedy family')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smd['title'].iloc[0], smd['soup'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обрати внимание, как выглядит запись о  фильме \"История игрушек\". В строке есть информация и об актерах, и о режиссере, и о жанре фильма, и про ключевые слова не забыли. Если для человека эта строка абсолютно бесполезная, то компьютер с ней быстро справится.\n",
    "\n",
    "___\n",
    "\n",
    "Помнишь в задании мы рассмотрели векторное представление текста, чтобы рекомендации заработали? Начинаем работу по преобразованию данных. Сейчас мы превратим этот текст в математический объект - **вектор**. Как ты знаешь, вектор это направленный отрезок. Это довольно простой объект, но для нас он будет ключевым элементом в системе контентных рекомендаций. \n",
    "\n",
    "Когда мы переведем описание каждого фильма в вектор, то как и с любыми другими векторами, ты, наконец, сможешь автоматически сравнивать фильмы между собой. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы преобразовать текстовое описание в вектор нам будет нужен объект **CountVectorizer**. Он умеет превращать огромный массив текстов в набор векторов. Посмотри на пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brown</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>fox</th>\n",
       "      <th>jumped</th>\n",
       "      <th>lazy</th>\n",
       "      <th>like</th>\n",
       "      <th>over</th>\n",
       "      <th>quick</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      brown  cat  dog  fox  jumped  lazy  like  over  quick\n",
       "Doc1      1    0    1    1       1     1     0     1      1\n",
       "Doc2      0    0    1    0       0     0     1     0      0\n",
       "Doc3      0    1    0    0       0     0     1     0      0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectors = vectorizer.fit_transform(['quick brown fox jumped over lazy dog', \n",
    "                          'i like dog', \n",
    "                          'i like cat'])\n",
    "pd.DataFrame(data=vectors.toarray(), \n",
    "             index=['Doc1', 'Doc2', 'Doc3'],\n",
    "             columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На входе было 3 предложения:\n",
    "- 'quick brown fox jumped over lazy dog', \n",
    "- 'i like dog',\n",
    "- 'i like cat'\n",
    "\n",
    "На выходе мы получили 3 вектора. Обрати внимание, как эти вектора выглядят. Координаты вектора - это слова, поэтому, если в предложении было слово **dog**, то в координате под названием **dog** будет число 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точно так же, чуть позже, ты превратишь все описания фильмов в вектора. Но есть ещё один вопрос, как сравнивать эти вектора?\n",
    "\n",
    "Простой вопрос, какое предложение больше похоже на **quick brown fox jumped over lazy dog**: \n",
    "    - i like dog\n",
    "    - i like cat\n",
    "Правильный ответ - i like dog. В исходном предложении есть слово dog, но нет слова cat.\n",
    "\n",
    "Чтобы это понять математически, используют скалярное произведение векторов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.26726124, 0.        ],\n",
       "       [0.26726124, 1.        , 0.5       ],\n",
       "       [0.        , 0.5       , 1.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция **cosine_similarity** вычислит скалярное произведение между всеми парами векторов. В примере у нас 3 вектора - 3 предложения, поэтому на выходе функции мы получим табличку размера **3x3**.  \n",
    "В первой строке таблицы записаны значения скалярных произведений между первым предложением и остальными двумя. Легко заметить, что расстояние вектора до самого себя = 1 (т.к. предложение на 100% совпадает с самим собой), расстояние до второго предложения = 0.26, до третьего = 0. То есть благодаря рассчитаным значениям мы сделаем вывод, что второе предложение гораздо ближе к первому, чем третье.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь давай провернём этот трюк с нашими фильмами Создай объект **CountVectorizer** и передай в функцию **fit_transform** все текстовые описания фильмов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9219, 21384)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = CountVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "count_matrix = count.fit_transform(smd['soup'])\n",
    "count_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ты получил матрицу размером **9219x21346** - это значит, что все **9219** фильмов превратились в векторы размером **21346** элементов. \n",
    "\n",
    "Заметь, что объект CountVectorizer дает возможность установить дополнительные настройки\n",
    " - параметр **ngram_range=(1,2)** помогает учитывать не только отдельные слова, но и пары слов\n",
    " - параметр **min_df=2** отфильтрует все слова, которые встречались меньше чем в двух фильмах\n",
    " \n",
    "Что нам даёт информация о размере матрицы? В нашем датасете **21346** уникальных слов и пар слов, которые встречались не менее чем в двух фильмах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь осталось вычислить скалярные произведения между всеми парами фильмов. Повтори все то же, что и в примере выше:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9219, 9219)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim = cosine_similarity(count_matrix, count_matrix)\n",
    "cosine_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В уменьшенном датасете было **9219** фильмов, поэтому матрица предсказаний имеет размеры **9219x9219**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция рекомендации фильма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ты уже сделал большую работу, теперь всё готово для написания функции рекомендации фильма. Тебе нужно написать функцию, которая принимает на вход фильм, а в ответ советует другие, похожие на него. Фильмы с самым минимальным расстоянием до фильма на входе в итоге и попадут в список рекомендаций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "smd = smd.reset_index()\n",
    "titles = smd['title']# Сохрани в переменную title колонку с названиями фильмов из датасета smd\n",
    "indices = pd.Series(smd.index, index=smd['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(title):\n",
    "    idx = indices[title]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:31]\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    return titles.iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ура, все готово! Проверим твою систему в работе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3899    The Lord of the Rings: The Fellowship of the Ring\n",
       "8833            The Hobbit: The Battle of the Five Armies\n",
       "4436                The Lord of the Rings: The Two Towers\n",
       "8537                  The Hobbit: The Desolation of Smaug\n",
       "5074        The Lord of the Rings: The Return of the King\n",
       "1693                                The Lord of the Rings\n",
       "8867                                             Warcraft\n",
       "477                                            The Shadow\n",
       "5852                                           The Hobbit\n",
       "2730                      Baby: Secret of the Lost Legend\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations('The Hobbit: An Unexpected Journey').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотри, самые похожие фильмы на кино \"The Hobbit: An Unexpected Journey\" тоже будут фильмами про хоббитов. Система использует описания фильмов, чтобы выдавать рекомендации — ты отлично справился со вторым заданием. Остается сделать последний шаг в финальном проекте, разработать систему коллаборативной фильтрации. Об этом мы поговорим в третьем задании. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
